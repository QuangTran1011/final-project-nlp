{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-18 14:16:17.117709: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-11-18 14:16:17.254923: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-11-18 14:16:17.255014: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-11-18 14:16:17.279414: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-18 14:16:17.348849: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-11-18 14:16:17.350380: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-18 14:16:18.374949: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/quangtran/anaconda3/envs/nlp/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from positional_embedding import PositionalEmbedding\n",
    "from bert import minBert\n",
    "import sentencepiece as spm\n",
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>usually , he would be tearing around the livin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>but just one look at a minion sent him practic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>that had been megan 's plan when she got him d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>he 'd seen the movie almost by mistake , consi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>she liked to think being surrounded by adults ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6999995</th>\n",
       "      <td>`` where is kristine ? ''</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6999996</th>\n",
       "      <td>`` what do you mean ? ''</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6999997</th>\n",
       "      <td>`` she 's gone .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6999998</th>\n",
       "      <td>no one knows where . ''</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6999999</th>\n",
       "      <td>`` gone ? ''</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7000000 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      text\n",
       "0        usually , he would be tearing around the livin...\n",
       "1        but just one look at a minion sent him practic...\n",
       "2        that had been megan 's plan when she got him d...\n",
       "3        he 'd seen the movie almost by mistake , consi...\n",
       "4        she liked to think being surrounded by adults ...\n",
       "...                                                    ...\n",
       "6999995                          `` where is kristine ? ''\n",
       "6999996                           `` what do you mean ? ''\n",
       "6999997                                   `` she 's gone .\n",
       "6999998                            no one knows where . ''\n",
       "6999999                                       `` gone ? ''\n",
       "\n",
       "[7000000 rows x 1 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlm_data = pd.read_csv('../data/bookcorpus.csv')\n",
    "mlm_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = '\\n\\n'.join(mlm_data['text'])\n",
    "# text[:300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from unicodedata import normalize\n",
    "# text = normalize('NFKC', text) \n",
    "\n",
    "# with open('../data/merged_text.txt', 'w') as fw:\n",
    "#     fw.write(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spm.SentencePieceTrainer.train(input='../data/merged_text.txt', model_prefix='minbert_bpe',vocab_size=30000, model_type='bpe', user_defined_symbols=\"[MASK]\", pad_id=0, unk_id=3, num_threads=14, minloglevel=1)\n",
    "# sp_bpe = spm.SentencePieceProcessor()\n",
    "# sp_bpe.load('minbert_bpe.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "s0 = mlm_data['text'][0]\n",
    "s1 = mlm_data['text'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-18 14:16:33.751442: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-11-18 14:16:33.772576: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2256] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")\n",
    "sentences = [s0, s1]  # Danh sách các câu\n",
    "s0_tokenized = tokenizer(sentences, return_tensors=\"tf\", add_special_tokens=True, padding=True, truncation=True, max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': <tf.Tensor: shape=(2, 18), dtype=int32, numpy=\n",
       "array([[  101,  2788,  1010,  2002,  2052,  2022, 13311,  2105,  1996,\n",
       "         2542,  2282,  1010,  2652,  2007,  2010, 10899,  1012,   102],\n",
       "       [  101,  2008,  2018,  2042, 12756,  1005,  1055,  2933,  2043,\n",
       "         2016,  2288,  2032,  5102,  3041,  1012,   102,     0,     0]],\n",
       "      dtype=int32)>, 'token_type_ids': <tf.Tensor: shape=(2, 18), dtype=int32, numpy=\n",
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "      dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(2, 18), dtype=int32, numpy=\n",
       "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0]],\n",
       "      dtype=int32)>}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s0_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLS Token ID: 101\n",
      "SEP Token ID: 102\n",
      "PAD Token ID: 0\n",
      "UNK Token ID: 100\n",
      "MASK Token ID: 103\n",
      "Vocab size: 30522\n"
     ]
    }
   ],
   "source": [
    "cls_token_id = tokenizer.cls_token_id\n",
    "sep_token_id = tokenizer.sep_token_id\n",
    "pad_token_id = tokenizer.pad_token_id\n",
    "unk_token_id = tokenizer.unk_token_id\n",
    "mask_token_id = tokenizer.mask_token_id\n",
    "vocab_size = tokenizer.vocab_size\n",
    "\n",
    "print(f\"CLS Token ID: {cls_token_id}\")\n",
    "print(f\"SEP Token ID: {sep_token_id}\")\n",
    "print(f\"PAD Token ID: {pad_token_id}\")\n",
    "print(f\"UNK Token ID: {unk_token_id}\")\n",
    "print(f\"MASK Token ID: {mask_token_id}\")\n",
    "print(f\"Vocab size: {vocab_size}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': <tf.Tensor: shape=(1, 2), dtype=int32, numpy=array([[103, 101]], dtype=int32)>, 'token_type_ids': <tf.Tensor: shape=(1, 2), dtype=int32, numpy=array([[0, 0]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(1, 2), dtype=int32, numpy=array([[1, 1]], dtype=int32)>}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s0_tokenized = tokenizer('[MASK]', '[CLS]', return_tensors=\"tf\", add_special_tokens=False)\n",
    "s0_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_mask(texts, \n",
    "                      noise=0.15, \n",
    "                      tokenizer=tokenizer):\n",
    "    encoded_texts = tokenizer(texts, return_tensors=\"tf\", add_special_tokens=True, padding=True, truncation=True, max_length=256)['input_ids']\n",
    "    inp_mask = np.random.rand(*encoded_texts.shape) < noise\n",
    "    inp_mask[encoded_texts == 101] = False\n",
    "    inp_mask[encoded_texts == 102] = False\n",
    "    inp_mask[encoded_texts == 0] = False\n",
    "    \n",
    "    labels = -1 * np.ones(encoded_texts.shape, dtype=int)\n",
    "    # Set labels for masked tokens\n",
    "    labels[inp_mask] = encoded_texts[inp_mask]\n",
    "\n",
    "    # Prepare input\n",
    "    encoded_texts_masked = np.copy(encoded_texts)\n",
    "    # Set input to [MASK] which is the last token for the 90% of tokens\n",
    "    # This means leaving 10% unchanged\n",
    "    inp_mask_2mask = inp_mask & (np.random.rand(*encoded_texts.shape) < 0.90)\n",
    "    encoded_texts_masked[\n",
    "        inp_mask_2mask\n",
    "    ] = tokenizer.mask_token_id  # mask token is the last in the dict\n",
    "\n",
    "    # Set 10% to a random token\n",
    "    inp_mask_2random = inp_mask_2mask & (np.random.rand(*encoded_texts.shape) < 1 / 9)\n",
    "    encoded_texts_masked[inp_mask_2random] = np.random.randint(\n",
    "        104, 200, inp_mask_2random.sum()\n",
    "    )\n",
    "    \n",
    "    # Prepare sample_weights to pass to .fit() method\n",
    "    sample_weights = np.ones(labels.shape)\n",
    "    sample_weights[labels == -1] = 0\n",
    "    \n",
    "    y_labels = np.copy(encoded_texts)\n",
    "\n",
    "    return encoded_texts_masked, y_labels, sample_weights\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b, c = tokenize_and_mask(s0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  101  2788  1010   103   103  2022 13311   103  1996  2542  2282  1010\n",
      "   2652  2007   103 10899  1012   102]]\n"
     ]
    }
   ],
   "source": [
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  101  2788  1010  2002  2052  2022 13311  2105  1996  2542  2282  1010\n",
      "   2652  2007  2010 10899  1012   102]]\n"
     ]
    }
   ],
   "source": [
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "x_masked_train, y_masked_labels, sample_weights = tokenize_and_mask(mlm_data['text'].values.tolist())\n",
    "\n",
    "mlm_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    (x_masked_train, y_masked_labels)\n",
    ")\n",
    "mlm_ds = mlm_ds.shuffle(1000).batch(BATCH_SIZE)\n",
    "mlm_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_bert_layer = minBert(\n",
    "    num_layers=4,  # Số lượng lớp encoder\n",
    "    d_model=256,   # Kích thước vector ẩn\n",
    "    num_heads=8,   # Số lượng head trong multi-head attention\n",
    "    dff=1024,      # Số lượng neuron trong feed-forward network\n",
    "    vocab_size=tokenizer.vocab_size,  # Kích thước từ vựng\n",
    "    dropout_rate=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlm_model = tf.keras.Sequential([\n",
    "    min_bert_layer,\n",
    "    tf.keras.layers.Dense(512, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(tokenizer.vocab_size, activation=\"softmax\")\n",
    "])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fuction = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=False,\n",
    "    ignore_class= -1,\n",
    ")\n",
    "\n",
    "mlm_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=5e-5),\n",
    "    loss=loss_fuction,\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "mlm_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlm_model.fit(mlm_ds, epochs=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
