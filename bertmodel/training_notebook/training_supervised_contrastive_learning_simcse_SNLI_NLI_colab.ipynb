{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.14",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 10026252,
          "sourceType": "datasetVersion",
          "datasetId": 6127937
        }
      ],
      "dockerImageVersionId": 30786,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "flyzPLPhZC4j",
        "outputId": "fcb75b3a-b443-4c6d-e3de-a3a0d092de7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from transformers import BertTokenizer\n",
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive/Colab Notebooks/bertmodel/new')\n",
        "from bert import minBert\n",
        "from bert import EncoderLayer, SelfAttention, FeedForward, BaseAttention\n",
        "from positional_embedding import PositionalEmbedding\n",
        "sys.path.append('/content/drive/MyDrive/Colab Notebooks/bertmodel/contrastive_learning')\n",
        "from contrastive_learning import SupervisedContrastiveModel"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-27T06:51:11.271096Z",
          "iopub.execute_input": "2024-11-27T06:51:11.272208Z",
          "iopub.status.idle": "2024-11-27T06:51:28.894602Z",
          "shell.execute_reply.started": "2024-11-27T06:51:11.272151Z",
          "shell.execute_reply": "2024-11-27T06:51:28.893597Z"
        },
        "id": "puroDX8OYzm6"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-27T06:51:31.566387Z",
          "iopub.execute_input": "2024-11-27T06:51:31.567028Z",
          "iopub.status.idle": "2024-11-27T06:51:32.825406Z",
          "shell.execute_reply.started": "2024-11-27T06:51:31.566991Z",
          "shell.execute_reply": "2024-11-27T06:51:32.824423Z"
        },
        "id": "ssRCUamsYzm7"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "min_bert_layer = tf.keras.models.load_model(\n",
        "    '/content/drive/MyDrive/Colab Notebooks/bertmodel/min_bert_layer.keras',\n",
        "    custom_objects={\n",
        "        \"minBert\": minBert,\n",
        "        \"EncoderLayer\": EncoderLayer,\n",
        "        \"SelfAttention\": SelfAttention,\n",
        "        \"FeedForward\": FeedForward,\n",
        "        \"BaseAttention\" : BaseAttention,\n",
        "        \"PositionalEmbedding\" : PositionalEmbedding\n",
        "    }\n",
        ")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-27T06:51:34.809393Z",
          "iopub.execute_input": "2024-11-27T06:51:34.809789Z",
          "iopub.status.idle": "2024-11-27T06:51:39.795112Z",
          "shell.execute_reply.started": "2024-11-27T06:51:34.809755Z",
          "shell.execute_reply": "2024-11-27T06:51:39.794315Z"
        },
        "id": "Dj8K_SyuYzm7"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "id": "RAih4Y7JaJVB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Load SNLI dataset\n",
        "dataset = load_dataset(\"1-800-SHARED-TASKS/SNLI-NLI\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-27T07:28:16.188780Z",
          "iopub.execute_input": "2024-11-27T07:28:16.189379Z",
          "iopub.status.idle": "2024-11-27T07:28:17.535273Z",
          "shell.execute_reply.started": "2024-11-27T07:28:16.189342Z",
          "shell.execute_reply": "2024-11-27T07:28:17.534575Z"
        },
        "id": "5X-aBgAbYzm8"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Tạo dữ liệu đầu vào (anchors, positives, negatives)\n",
        "def prepare_data(dataset, tokenizer, batch_size=32, max_length=256):\n",
        "    anchors = []\n",
        "    positives = []\n",
        "    negatives = []\n",
        "\n",
        "    print(dataset[0])\n",
        "\n",
        "    step = 1\n",
        "\n",
        "    for i in range(0, len(dataset) - 2, step + 1):  # bước nhảy ở cuối mỗi nhóm (i+3)\n",
        "        sample_1 = dataset[i]         # Phần tử đầu tiên\n",
        "        sample_2 = dataset[i + 1]     # Phần tử thứ hai\n",
        "        sample_3 = dataset[i + 2]     # Phần tử thứ ba\n",
        "\n",
        "        group = [sample_1, sample_2, sample_3]\n",
        "        for sample in group:\n",
        "          if sample['label'] == 0:\n",
        "              anchors.append(sample['premise'])\n",
        "              positives.append(sample['hypothesis'])\n",
        "          elif sample['label'] == 2:\n",
        "              negatives.append(sample['hypothesis'])\n",
        "\n",
        "    min_len = min(len(anchors), len(positives), len(negatives))\n",
        "    anchors = anchors[:min_len]\n",
        "    positives = positives[:min_len]\n",
        "    negatives = negatives[:min_len]\n",
        "\n",
        "    print(\"Tổng số anchors:\", len(anchors))\n",
        "    print(\"Tổng số positives:\", len(positives))\n",
        "    print(\"Tổng số negatives:\", len(negatives))\n",
        "    # Kiểm tra xem có giá trị None hay không trong dữ liệu đầu vào\n",
        "    if any(val is None for val in anchors + positives + negatives):\n",
        "        print(\"Cảnh báo: Tìm thấy None trong các dữ liệu đầu vào!\")\n",
        "\n",
        "    # Tiến hành tokenization\n",
        "    def tokenize_texts(texts):\n",
        "        encodings = tokenizer(texts, padding='max_length', truncation=True, max_length=max_length, return_tensors=\"tf\")\n",
        "        return encodings\n",
        "\n",
        "    anchor_encodings = tokenize_texts(anchors)\n",
        "    positive_encodings = tokenize_texts(positives)\n",
        "    negative_encodings = tokenize_texts(negatives)\n",
        "\n",
        "    # Kiểm tra xem tokenization có trả về None không\n",
        "    if anchor_encodings is None or positive_encodings is None or negative_encodings is None:\n",
        "        print(\"Cảnh báo: Tokenizer không thành công với một số mẫu!\")\n",
        "\n",
        "    # Tạo tf.data.Dataset\n",
        "    dataset = {\n",
        "        'anchors': anchor_encodings['input_ids'],\n",
        "        'positives': positive_encodings['input_ids'],\n",
        "        'negatives': negative_encodings['input_ids']\n",
        "    }\n",
        "\n",
        "    return dataset\n",
        "\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-27T06:54:55.333337Z",
          "iopub.execute_input": "2024-11-27T06:54:55.334037Z",
          "iopub.status.idle": "2024-11-27T06:55:19.962533Z",
          "shell.execute_reply.started": "2024-11-27T06:54:55.334005Z",
          "shell.execute_reply": "2024-11-27T06:55:19.961428Z"
        },
        "id": "n0j6FQPNYzm8"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = prepare_data(dataset['train'].select(range(200000)), tokenizer)"
      ],
      "metadata": {
        "id": "-iOjLffRaUfI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_tf_dataset(train_data, batch_size=32):\n",
        "    # Chuyển thành tf.data.Dataset\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((train_data['anchors'], train_data['positives'], train_data['negatives']))\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    return dataset\n",
        "\n",
        "# Tạo dataset cho huấn luyện\n",
        "tf_train_dataset = create_tf_dataset(train_dataset)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-27T06:55:28.563425Z",
          "iopub.execute_input": "2024-11-27T06:55:28.563796Z",
          "iopub.status.idle": "2024-11-27T06:55:28.574127Z",
          "shell.execute_reply.started": "2024-11-27T06:55:28.563766Z",
          "shell.execute_reply": "2024-11-27T06:55:28.573216Z"
        },
        "id": "p4eVU_xdYzm8"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "model = SupervisedContrastiveModel(min_bert_layer)\n",
        "\n",
        "# Đặt các tham số huấn luyện\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5))\n",
        "\n",
        "# Huấn luyện mô hình\n",
        "model.fit(tf_train_dataset, epochs=3)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-27T06:55:30.911125Z",
          "iopub.execute_input": "2024-11-27T06:55:30.911996Z",
          "iopub.status.idle": "2024-11-27T07:09:34.028440Z",
          "shell.execute_reply.started": "2024-11-27T06:55:30.911954Z",
          "shell.execute_reply": "2024-11-27T07:09:34.027438Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NUbQNOGxYzm9",
        "outputId": "41b09634-9235-4c0d-cba3-bf90ab007761"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:934: UserWarning: Layer 'query' (of type EinsumDense) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:934: UserWarning: Layer 'key' (of type EinsumDense) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:934: UserWarning: Layer 'value' (of type EinsumDense) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:934: UserWarning: Layer 'self_attention' (of type SelfAttention) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:934: UserWarning: Layer 'sequential' (of type Sequential) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:934: UserWarning: Layer 'feed_forward' (of type FeedForward) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:934: UserWarning: Layer 'encoder_layer' (of type EncoderLayer) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:934: UserWarning: Layer 'self_attention_1' (of type SelfAttention) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:934: UserWarning: Layer 'sequential_1' (of type Sequential) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:934: UserWarning: Layer 'feed_forward_1' (of type FeedForward) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:934: UserWarning: Layer 'encoder_layer_1' (of type EncoderLayer) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:934: UserWarning: Layer 'self_attention_2' (of type SelfAttention) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:934: UserWarning: Layer 'sequential_2' (of type Sequential) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:934: UserWarning: Layer 'feed_forward_2' (of type FeedForward) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:934: UserWarning: Layer 'encoder_layer_2' (of type EncoderLayer) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:934: UserWarning: Layer 'self_attention_3' (of type SelfAttention) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:934: UserWarning: Layer 'sequential_3' (of type Sequential) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:934: UserWarning: Layer 'feed_forward_3' (of type FeedForward) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:934: UserWarning: Layer 'encoder_layer_3' (of type EncoderLayer) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:391: UserWarning: `build()` was called on layer 'supervised_contrastive_model', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m3118/3118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1908s\u001b[0m 599ms/step - loss: 2.2625\n",
            "Epoch 2/3\n",
            "\u001b[1m3118/3118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1901s\u001b[0m 593ms/step - loss: 1.8796\n",
            "Epoch 3/3\n",
            "\u001b[1m3118/3118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1884s\u001b[0m 593ms/step - loss: 1.7276\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x78b850299390>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "min_bert_layer.save(\"min_bert_layer_after_nli.keras\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-27T07:30:54.718100Z",
          "iopub.execute_input": "2024-11-27T07:30:54.718757Z",
          "iopub.status.idle": "2024-11-27T07:30:55.079340Z",
          "shell.execute_reply.started": "2024-11-27T07:30:54.718719Z",
          "shell.execute_reply": "2024-11-27T07:30:55.078305Z"
        },
        "id": "tVoXNDjDYzm_"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}